<h1 align="center"> ğŸŒ¿ Groundnut Leaf Disease Detection Using Vision Transformer (ViT) in PyTorch </h1>

This repository provides a complete pipeline for detecting groundnut (peanut) leaf diseases using Vision Transformer (ViT) models. The solution is developed using PyTorch and fine-tuned on a custom high-resolution dataset of healthy and diseased groundnut leaves collected from West Bengal, India.

## ğŸ§¾ Project Overview
<div align="center">

| Attribute              | Description                                                                 |
|------------------------|-----------------------------------------------------------------------------|
| ğŸ“š Framework           | PyTorch, Torchvision, Timm                                                   |
| ğŸ§  Models Used         | EfficientNetB0, MobileNetV2, ShuffleNetV2, Vision Transformer (ViT)          |
| ğŸ“· Input Format        | JPEG Images (224Ã—224 px)                                                     |
| ğŸ¯ Output              | 5-Class Image Classification                                                 |
| ğŸ§ª File                | `vit-code-pretrained.ipynb`                                                  |
| ğŸ“ Dataset Source      | [Mendeley Dataset](https://data.mendeley.com/datasets/x6x5jkk873/1)          |

</div>

## ğŸ—‚ Dataset Overview

This dataset contains a total of **1,720 images** of groundnut leaves collected from **Purba Medinipur, West Bengal, India**, across five categories. Images are high-resolution (~4624Ã—3472 px) and organized into class-wise folders.

### ğŸ“Š Class Distribution

<h3 align="center">ğŸ“Š Class Distribution</h3>

<table align="center">
  <tr>
    <td align="center"><img src="images/Healthy.jpg" width="120px"></td>
    <td align="center"><img src="images/Alternaria Leaf Spot.jpg" width="120px"></td>
    <td align="center"><img src="images/Leaf Spot.jpg" width="120px"></td>
    <td align="center"><img src="images/Rust.jpg" width="120px"></td>
    <td align="center"><img src="images/Rosette.jpg" width="120px"></td>
  </tr>
  <tr>
    <td align="center"><b>Healthy</b></td>
    <td align="center"><b>Alternaria Leaf Spot</b></td>
    <td align="center"><b>Leaf Spot</b></td>
    <td align="center"><b>Rust</b></td>
    <td align="center"><b>Rosette</b></td>
  </tr>
</table>



- Total Images: **1,720**
- Format: `.jpg` (~4624Ã—3472 resolution)
- Directory Structure: Class-wise folders
- Split: 80% train / 20% test using `Subset` and `train_test_split`



## ğŸ§ª Notebook Workflow
<div align="center">
  
| Step                | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| ğŸ“¥ Data Loading      | Loaded with `torchvision.datasets.ImageFolder` and custom transforms        |
| ğŸ§¼ Preprocessing      | Resize â†’ ToTensor â†’ Normalize (ImageNet stats)                             |
| ğŸ§  Model              | Vision Transformer `vit_b_16`, pre-trained on ImageNet, fine-tuned head     |
| ğŸ› ï¸ Fine-Tuning        | Only classification head (`model.heads.head`) is trainable                 |
| ğŸ” Training           | 20 epochs using `CrossEntropyLoss` + `Adam` optimizer                       |
| ğŸ“Š Evaluation         | Accuracy, precision, recall, F1-score, confusion matrix                     |
| ğŸ” Inference          | Custom `predict_image()` function for single image classification           |

</div>


## âš™ï¸ Model Configuration
<div align="center">

| Parameter             | Value              |
|------------------------|--------------------|
| Input Image Size       | 224 Ã— 224          |
| Pretrained Weights     | ImageNet-1k        |
| Trainable Layers       | Classification Head only |
| Epochs                 | 20                 |
| Batch Size             | 32                 |
| Learning Rate          | 0.001              |
| Optimizer              | Adam               |
| Loss Function          | CrossEntropyLoss   |

</div>


## ğŸ“ˆ Epoch-wise Performance
<div align="center">

| Metric               | Value     |
|----------------------|-----------|
| ğŸ§ª Final Test Accuracy  | **96.80%** |
| ğŸŒŸ Peak Test Accuracy   | **97.38%** (Epoch 19) |
| ğŸ“‰ Final Test Loss      | **0.1317** |
| ğŸ” Total Epochs         | 20        |

</div>

---

## ğŸ” Confusion Matrix & Classification Report

A visual representation of model performance across classes:

<p align="center">
  <img src="images/Confusion_Matrix.png" width="500px">
</p>

---

### ğŸ“‰ Training & Validation Loss Curve

The loss curve illustrates how the model converges during training:

<p align="center">
  <img src="images/Loss_Curve.png" width="500px">
</p>

---

## âœ… Conclusion

This project successfully demonstrates how Vision Transformer (ViT), when fine-tuned on a high-resolution groundnut leaf dataset, can achieve high classification performance in detecting common diseases such as:

- **Alternaria Leaf Spot**
- **Leaf Spot**
- **Rust**
- **Rosette**
- **Healthy**

The model achieves a test accuracy of up to **97.38%**, showing strong potential for real-world agricultural applications, especially for early-stage disease detection in crops.

---

## ğŸ”® Future Improvements

Some suggested enhancements to this project include:

- ğŸ–¼ï¸ **Real-time leaf disease detection** using OpenCV and webcam integration  
- ğŸ“ˆ **Data augmentation strategies** like mixup, cutmix, and elastic distortions  
- ğŸ§  **Model explainability** with Grad-CAM to highlight decision areas on leaves  
- â˜ï¸ **Deploy the model** via Flask, FastAPI, or Streamlit for web/mobile apps  
- ğŸ” **Automated retraining** pipeline using newly collected field data  
- ğŸ§ª Experiment with **ViT variants** like `vit_b_32`, `deit`, or `swin_transformer`

---

## ğŸš€ How to Execute This Project

Follow the steps below to clone and run this project on your machine:

#### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/groundnut-leaf-disease-vit.git
cd groundnut-leaf-disease-vit
```
---
2ï¸âƒ£ Install Required Libraries
Make sure Python 3.8+ is installed. Then install the dependencies using:

```bash
pip install -r requirements.txt
```
âœ… All required packages (including PyTorch, torchvision, timm, matplotlib, scikit-learn, etc.) are listed in the requirements.txt.

3ï¸âƒ£ Download the Dataset
Go to the official Mendeley Dataset, download it, and extract the contents.
[Mendeley Dataset](https://data.mendeley.com/datasets/x6x5jkk873/1)

---

4ï¸âƒ£ Run the Notebook
Use Jupyter Notebook or any compatible environment to execute the code:
```bash
jupyter notebook vit-code-pretrained.ipynb
```
The notebook covers:

âœ… Data loading and preprocessing  
âœ… Vision Transformer (ViT) fine-tuning  
âœ… Model evaluation with accuracy, loss, F1-score  
âœ… Visualization of confusion matrix and loss curve  
âœ… Single image prediction with `predict_image()`

---


## ğŸ“Š Features

- Vision Transformer (ViT) model implemented from scratch  
- Patch embedding layer to convert images into token sequences  
- Positional encoding and multi-head self-attention  
- Configurable training loop with optimizer and loss function  
- Training and validation loss/accuracy plots  
- Confusion matrix and classification report  

## ğŸ”§ Future Improvements

- Add pretrained ViT support from Hugging Face or torchvision  
- Expand to larger datasets (CIFAR-100, ImageNet)  
- Integrate with mixed precision training (AMP)  
- Add experiment logging (TensorBoard, Weights & Biases)  
- Deploy model via a web app (Streamlit/Flask)  

## ğŸ–¼ï¸ Sample Outputs

_Add images here, such as training/validation accuracy plots, loss curves, and confusion matrices._

## ğŸ¤ Contributing

Contributions are welcome! Feel free to fork the repo and submit a pull request.  
For any issues or feature requests, please open an [issue](https://github.com/your-username/your-repo/issues).
